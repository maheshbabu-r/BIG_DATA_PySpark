{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reviews_ Analysis_Spark.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maheshbabu-r/BIG_DATA_PySpark/blob/main/Reviews__Analysis_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlsv9yGB2I81"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz  # supress download output use -q\n",
        "\n",
        "!tar -zxvf spark-3.1.2-bin-hadoop3.2.tgz | grep \"something\" 2>/dev/null #Suppress tar output ---| grep \"something\" 2>/dev/null--- add after file_name\"\n",
        "\n",
        "!pip -q install findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6RZ_81C3IkA"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf\n",
        "from pyspark import SparkContext\n",
        "\n",
        "#from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "\n",
        "# or import like this\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "spark = SparkSession.builder.appName(\"Analysis Reviews\").getOrCreate()\n",
        "print(spark.sparkContext.appName)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7gQXmcK3uCd"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/maheshbabu-r/BIG_DATA/main/Hadoop%20Datasets/senticnet4.txt\n",
        "\n",
        "!wget -q https://github.com/maheshbabu-r/BIG_DATA/blob/main/Hadoop%20Datasets/amazondataset.rar?raw=true\n",
        "\n",
        "# rename file for better understanding\n",
        "!mv amazondataset.rar?raw=true amazondataset.rar\n",
        "\n",
        "!unrar x /content/amazondataset.rar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "garVvL3yAQF4"
      },
      "source": [
        "df=spark.read.csv(header=True,inferSchema=True,path=\"/content/amazondataset.csv\")\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7LAyi_qDQ9b"
      },
      "source": [
        "df1=spark.read.option(\"sep\", \"\\t\").csv(\"/content/senticnet4.txt\")\n",
        "\n",
        "df1.printSchema()\n",
        "\n",
        "df1.show(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQHjRy2XGTzC"
      },
      "source": [
        "schema = StructType([ \\\n",
        "    StructField(\"_c0\",StringType(),True), \\\n",
        "    StructField(\"_c1\",StringType(),True),\\\n",
        "    StructField(\"_c2\",FloatType(), True) \\\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSTqg-C9GkbB"
      },
      "source": [
        "df1=spark.read.option(\"sep\", \"\\t\").csv(\"/content/senticnet4.txt\",schema=schema)\n",
        "\n",
        "df1.printSchema()\n",
        "\n",
        "df1.show(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAKtmHxLDQ6T"
      },
      "source": [
        "df1 = df1.withColumnRenamed(\"_c0\",\"words\") \\\n",
        "    .withColumnRenamed(\"_c1\",\"sentiment\") \\\n",
        "    .withColumnRenamed(\"_c2\",\"value\")\n",
        "df1.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbBeoUmtDQ3Z"
      },
      "source": [
        "df1.show(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7n88GV9DQ0i"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIU6eYn8DQxZ"
      },
      "source": [
        "sns.boxplot(x=df1.toPandas()[\"value\"].values.tolist())\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwiUi4njAp9v"
      },
      "source": [
        "x=df1.toPandas()[\"words\"].values.tolist()\n",
        "y=df1.toPandas()[\"value\"].values.tolist()\n",
        "ratings=dict(zip(x,y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA36wrI3SLSp"
      },
      "source": [
        "def sum_of_list(l):\n",
        "  total = 0\n",
        "  for val in l:\n",
        "    total = total + val\n",
        "  return total\n",
        "\n",
        "def reviews_value(reviews):\n",
        "  y=''.join(e.lower() for e in reviews if (e.isalnum() or e.isspace()))\n",
        "  y=[x for x in y.split(\" \")]\n",
        "  z=[ratings.get(x) for x in y]\n",
        "  final=[(0 if x==None else x) for x in z]\n",
        "  return sum_of_list(final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ3ma3h2fWhQ"
      },
      "source": [
        "texts=df.toPandas()[\"reviewstext\"].values.tolist()\n",
        "values=[reviews_value(x) for x in texts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7vDj6gvjQAL"
      },
      "source": [
        "def add_labels(indx):\n",
        "    return values[indx-1] # since row num begins from 1\n",
        "labels_udf = udf(add_labels, DoubleType())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVRo9VqwmBj7"
      },
      "source": [
        "df.createOrReplaceTempView('A')\n",
        "B = spark.sql('select row_number() over(order by \"mahesh\" ) as num, * from A')\n",
        "\n",
        "B.show(5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQrMCZyDnv-O"
      },
      "source": [
        "new_df = B.withColumn('Rating', labels_udf('num'))\n",
        "new_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSVFzKL1pb-p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}